{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67e5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the HTML content\n",
    "file_path = '5_REVIEWS.html'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "\n",
    "data = []\n",
    "review_list_container = soup.find('div', {'class': 'Responsesstyled__StyledList-sc-150koqm-5 ca-dtrb'})\n",
    "\n",
    "if review_list_container:\n",
    "    # Find all individual reviews within the container\n",
    "    review_items = review_list_container.find_all('div', recursive=False)\n",
    "\n",
    "    for review in review_items:\n",
    "        # Extract the name of the review\n",
    "        name_tag = review.find('a', {'data-test': 'link-text'})\n",
    "        name = name_tag.text.strip() if name_tag else 'N/A'\n",
    "        \n",
    "        # Extract the mark\n",
    "        mark_tag = review.find('div', {'class': 'Grade__sc-m0t12o-0 gJnnyh'})\n",
    "        mark = mark_tag['value'] if mark_tag else 'N/A'\n",
    "        \n",
    "        # Extract the review text\n",
    "        review_text_tag = review.find('div', {'class': 'Responsesstyled__StyledItemText-sc-150koqm-3 iPpiJn'})\n",
    "        review_text_link = review_text_tag.find('a', {'data-gtm-click': True}) if review_text_tag else None\n",
    "        review_text = review_text_link.text.strip() if review_text_link else 'N/A'\n",
    "        \n",
    "        data.append([name, mark, review_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf3fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data to CSV file\n",
    "csv_file_path = '5reviews_data.csv'\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Mark', 'Review'])\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Data saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5788ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "dataset1 = pd.read_csv('negative_reviews_data.csv')\n",
    "dataset2 = pd.read_csv('2reviews_data.csv')\n",
    "dataset3 = pd.read_csv('3reviews_data.csv')\n",
    "dataset4 = pd.read_csv('4reviews_data.csv')\n",
    "dataset5 = pd.read_csv('5REVIEWS_data.csv')\n",
    "\n",
    "# Concatenate the datasets\n",
    "combined_dataset = pd.concat([dataset1, dataset2, dataset3, dataset4, dataset5])\n",
    "\n",
    "# Save the combined dataset to a new CSV file\n",
    "combined_dataset.to_csv('combined_reviews.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a019538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('combined_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154cc755",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a134c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7043d5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a7dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241503a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('validation_data2.csv', sep = ';')\\\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2798d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7bf6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['Mark', 'Sentiment', 'model_mark']).size().reset_index(name='Count')\n",
    "\n",
    "# Print the results\n",
    "print(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5158cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model_mark and human_mark are categorical with the same labels\n",
    "df['Sentiment'] = df['Sentiment'].astype('category')\n",
    "df['human_mark'] = df['human_mark'].astype('category')\n",
    "\n",
    "\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score\n",
    "accuracy = accuracy_score(df['human_mark'], df['Sentiment'])\n",
    "precision = precision_score(df['human_mark'], df['Sentiment'], average='weighted')\n",
    "recall = recall_score(df['human_mark'], df['Sentiment'], average='weighted')\n",
    "f1 = f1_score(df['human_mark'], df['Sentiment'], average='weighted')\n",
    "\n",
    "# Print the classification report\n",
    "report = classification_report(df['human_mark'], df['Sentiment'], target_names=['2', '0', '1'])\n",
    "print(report)\n",
    "\n",
    "# Print overall metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a2398",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "barplot = sns.barplot(x='Mark', y='Count', hue='model_mark', data=grouped, palette='viridis')\n",
    "\n",
    "# Add annotations to the bars\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(format(p.get_height(), '.0f'),\n",
    "                     (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                     ha='center', va='center', \n",
    "                     xytext=(0, 9), textcoords='offset points')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Count of Each Model Sentiment per Banki.ru Review Mark')\n",
    "plt.xlabel('Mark (Star)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Place the legend outside the plot\n",
    "plt.legend(title='Model sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_plot_path = 'bankiru_mark_VS_sentiment.png'\n",
    "\n",
    "plt.savefig(output_plot_path)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved to {output_plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Mark'] = df['Mark'].astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed79157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_palette = {\n",
    "    'negative': sns.color_palette(\"hls\", 8)[0],  # Red\n",
    "    'neutral': sns.color_palette(\"hls\", 8)[1],   # Yellow\n",
    "    'positive': sns.color_palette(\"hls\", 8)[2]   # Green\n",
    "}\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "barplot = sns.barplot(x='Mark', y='Count', hue='model_mark', data=grouped, palette=custom_palette)\n",
    "\n",
    "# Add annotations to the bars\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(format(p.get_height(), '.0f'),\n",
    "                     (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                     ha='center', va='center', \n",
    "                     xytext=(0, 9), textcoords='offset points')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Count of Each Model Sentiment per Banki.ru Review Mark(star)')\n",
    "plt.xlabel('Number of Stars given by the User')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Place the legend outside the plot\n",
    "plt.legend(title='Model sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "output_plot_path = 'bankiru_mark_VS_sentiment777.png'\n",
    "\n",
    "plt.savefig(output_plot_path)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved to {output_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "def expected_sentiment(star):\n",
    "    if star in [1, 2, 3]:\n",
    "        return 'negative'\n",
    "    elif star == 3:\n",
    "        return ['neutral']\n",
    "    elif star in [4, 5]:\n",
    "        return 'positive'\n",
    "    \n",
    "df['expected_sentiment'] = df['Mark'].apply(expected_sentiment)\n",
    "# Calculate accuracy, precision, recall, and F1-score\n",
    "accuracy = accuracy_score(df['expected_sentiment'], df['model_mark'])\n",
    "precision = precision_score(df['expected_sentiment'], df['model_mark'], average='weighted')\n",
    "recall = recall_score(df['expected_sentiment'], df['model_mark'], average='weighted')\n",
    "f1 = f1_score(df['expected_sentiment'], df['model_mark'], average='weighted')\n",
    "\n",
    "\n",
    "\n",
    "# Print overall metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef2d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
